{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset  \n",
    "For the dataset, we use [Sentiment140](http://help.sentiment140.com/for-students)  \n",
    "And we will build a simple model that decides positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "import zipfile\n",
    "import urllib.request\n",
    "if not os.path.exists(\"dataset\"):\n",
    "    os.makedirs(\"dataset\")\n",
    "if not os.path.exists(os.path.join(\"dataset\", \"sentiment140\")):\n",
    "    urllib.request.urlretrieve(\"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\", os.path.join(\"dataset\", \"sentiment140.zip\"))\n",
    "    with zipfile.ZipFile(os.path.join(\"dataset\", \"sentiment140.zip\"), 'r') as inFile:\n",
    "        inFile.extractall(os.path.join(\"dataset\", \"sentiment140\"))\n",
    "    os.remove(os.path.join(\"dataset\", \"sentiment140.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_TRAINING = os.path.join(\"dataset\", \"sentiment140\", \"training.1600000.processed.noemoticon.csv\")\n",
    "FILE_PATH_PROCESSED = os.path.join(\"dataset\", \"sentiment140\", \"processed.csv\")\n",
    "FILE_PATH_TEST_NO_USE = os.path.join(\"dataset\", \"sentiment140\", \"testdata.manual.2009.06.14.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing  \n",
    "1. remove extra space, characters, and process @words  \n",
    "2. pad the sentences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FILE_PATH_PROCESSED):\n",
    "    tmp = pd.read_csv(FILE_PATH_TRAINING, names=[\"Target\", \"ID\", \"Date\", \"QueryInfo\", \"UserName\", \"Text\"], encoding=\"latin-1\")\n",
    "    tmp.to_csv(FILE_PATH_PROCESSED, encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(FILE_PATH_PROCESSED, encoding=\"utf-8\")\n",
    "training_data = training_data[[\"Target\", \"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "ch_range = list(range(97, 123)) + list(range(65, 91)) + [ord(' '), ord('\\'')]\n",
    "def process_str(raw_string):\n",
    "    global ch_range\n",
    "    global stemmer\n",
    "    # first remove url, @username, etc\n",
    "    raw_string = re.sub(r\"(@|#)([A-Z]|[a-z]|[0-9]|_)+\", \"\", raw_string)\n",
    "    raw_string = re.sub(r\"(http|https)://([A-Z]|[a-z]|[0-9]|/|\\.)+\", \"\", raw_string)\n",
    "    # remove characters other than [a-z][A-Z][0-9]['!?] or empty space\n",
    "    new_string = \"\".join([ch.lower() if ord(ch) in ch_range else ' ' for ch in list(raw_string)])\n",
    "    # remove extra space, and also convert plural form to singular\n",
    "    new_string = new_string.strip()\n",
    "    new_string = \" \".join([stemmer.stem(word) for word in new_string.split()])\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    awww that a bummer you shoulda got david carr ...\n",
       "1    is upset that he can't updat his facebook by t...\n",
       "2    i dive mani time for the ball manag to save th...\n",
       "3         my whole bodi feel itchi and like it on fire\n",
       "4    no it not behav at all i'm mad whi am i here b...\n",
       "5                                   not the whole crew\n",
       "6                                           need a hug\n",
       "7    hey long time no see yes rain a bit onli a bit...\n",
       "8                             nope they didn't have it\n",
       "9                                         que me muera\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[\"Text\"] = training_data[\"Text\"].apply(process_str)\n",
    "training_data[\"Text\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww that a bummer you shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't updat his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i dive mani time for the ball manag to save th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no it not behav at all i'm mad whi am i here b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>hey long time no see yes rain a bit onli a bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>que me muera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                               Text\n",
       "0       0  awww that a bummer you shoulda got david carr ...\n",
       "1       0  is upset that he can't updat his facebook by t...\n",
       "2       0  i dive mani time for the ball manag to save th...\n",
       "3       0       my whole bodi feel itchi and like it on fire\n",
       "4       0  no it not behav at all i'm mad whi am i here b...\n",
       "5       0                                 not the whole crew\n",
       "6       0                                         need a hug\n",
       "7       0  hey long time no see yes rain a bit onli a bit...\n",
       "8       0                           nope they didn't have it\n",
       "9       0                                       que me muera"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty string rows, or with only one word\n",
    "training_data.drop(training_data[\"Text\"][training_data[\"Text\"] == \"\"].index, inplace=True)\n",
    "training_data = training_data[training_data[\"Text\"].str.contains(\" \")]\n",
    "training_data.drop_duplicates(inplace=True)\n",
    "training_data.reset_index(drop=True)\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(os.path.join(\"dataset\", \"sentiment140\", \"final.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = training_data[\"Text\"].copy()\n",
    "y_train = training_data[\"Target\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_MAXLEN = 45\n",
    "MAX_FEATURES = 20000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(X_train_processed)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_processed)\n",
    "X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, padding=\"post\", maxlen=PAD_MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219141"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "TOKEN_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1516624, 45)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 481,   13,    4, 1175,    8, 3114,   46,  827, 7365,   12, 1772,\n",
       "          27,    2,   35,    5,  317,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [   9,  711,   13,   91,   72,  321,  192,  550,  129,  465,    5,\n",
       "           7,  301,  339,   87,    4, 1094,  150,   42,  275, 1097,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1516624,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_processed = y_train.replace(4, 1).to_numpy().ravel()\n",
    "y_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   22   444  1586    30   151    52   562    14  1632   129  2052  2773\n",
      "    481     1    55   155     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0]\n",
      " [  159   225  1007   158    75   325   178    10  5130     1  1078    49\n",
      "     45     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0]\n",
      " [   20   275   217  3913   125    87     1    15   206    12    33     2\n",
      "     35    34     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0]\n",
      " [   17    49    89   113    13   249    27   280   359     6   878    88\n",
      "     85    52  5561     7   318    87    91    85    11   113   144    20\n",
      "    191    10     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0]\n",
      " [  218  4857  1933   621    30   355     9  1579     7   581   129  5097\n",
      "  19639    39  1118   801   227     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0]]\n",
      "[0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# shuffle the dataset once to prepare for training\n",
    "index_permut = np.random.permutation(len(y_train_processed))\n",
    "X_train_final = np.array(X_train_pad)[index_permut]\n",
    "y_train_final = np.array(y_train_processed)[index_permut]\n",
    "print(X_train_final[:5])\n",
    "print(y_train_final[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"dataset\", \"sentiment140\", \"data.pickle\"), \"wb\") as outFile:\n",
    "    pickle.dump([X_train_final, y_train_final, tokenizer], outFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
